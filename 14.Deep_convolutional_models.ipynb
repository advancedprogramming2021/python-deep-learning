{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度卷积网络模型\n",
    "\n",
    "卷积神经网络是第一个解决重要商业应用的神经网络，并且仍然是当今深度学习商业应用的前沿，迄今为止已经提出了各种网络结构。经典卷积神经网络模型有：\n",
    "\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- VGG\n",
    "- GoogLeNet\n",
    "- ResNet\n",
    "- ResNeXt\n",
    "- DenseNet\n",
    "- SENet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "\n",
    "在 20 世纪 90 年代， Yann Lecun 领导的 AT&T 神经网络研究小组开发了一个用于识别手写数字的卷积网络 LeNet-5，用来识别支票上的手写数字。它有连续的卷积层和下采样层，最后经全连接层输出预测结果。\n",
    "\n",
    "输入层输入原始图像，原始图像的分辨率为 $32\\times32$个像素。然后，后面的隐藏层在卷积和下采样之间交替进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/chapter14/LeNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C1 层是卷积层，卷积核尺寸为 $5\\times 5$，输出了六个特征图（feature map），每个特征图尺寸是 $28\\times28$。参数个数为 $(5\\times5+1)\\times6=156$\n",
    "\n",
    "- S2 层是下采样层，它将局部像素值平均化来实现下采样。参数个数为 $2\\times6=12$\n",
    "\n",
    "![img](images/chapter14/LeNet_subsampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C3 层是卷积层，有 16 个卷积核，卷积核尺寸为 $5\\times5$，参数个数为 $1516$。需要注意的是，C3 与 S2 并不是全连接而是部分连接，C3 的特征图有些与 S2 三层特征图连接、有些四层、甚至达到6层，目的是通过这种方式提取更多特征，连接的规则如下表所示：\n",
    "\n",
    "![img](images/chapter14/LeNet_conv.png)\n",
    "\n",
    "- S4 层与 S2 层一样，也是下采样层，参数个数为 $2\\times16 = 32$\n",
    "\n",
    "- C5 层为卷积层，卷积核尺寸为 $5\\times5$，输出图像的宽高尺寸为 $(1,1)$，输出恰好可以视为一维向量。该层参数个数为 $(16\\times5\\times5+1)\\times120=48120$\n",
    "\n",
    "- F6 层为全连接层，包含 84 个神经元，参数个数为 $(120+1)\\times84=10164$\n",
    "\n",
    "- Output 层也是全连接层，包含 10 个神经元，分别代表数字 0 到 9 的预测输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5共包含约六万个可训练网络参数，和现在的卷积神经网络相比，LeNet-5 有几个不同点：\n",
    "\n",
    "- 激活函数\n",
    "\n",
    "      LeNet 中使用 Sigmoid 函数，而现在的 CNN 中主要使用 ReLU 函数\n",
    "- 下采样\n",
    "\n",
    "      原始的 LeNet 中使用下采样（Subsampling）缩小中间数据的大小，而现在的 CNN 中使用池化是主流\n",
    "\n",
    "由于当时缺乏大规模训练数据，计算机的计算能力较弱，LeNet-5 应用于其它复杂问题的处理结果并不理想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [AlexNet](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 出现在上个世纪九十年代，虽然展现了卷积神经网络的强大特征提取能力，但是迫于计算能力匮乏以及种种复杂的现实场景限制，只能在一些特定领域应用。同时，随着 SVM 等手工设计特征的机器学习算法飞速发展，LeNet-5 并没有形成广泛的应用。\n",
    "\n",
    "随着 ReLU 与 Dropout 的提出，以及 GPU 带来算力突破和互联网时代大数据的爆发，卷积神经网络带来历史的突破，AlexNet 网络的提出让深度学习走上人工智能的最前端。AlexNet 网络的作者是多伦多大学的 Alex Krizhevsky, Ilya Sutskever, **Geoffrey Hinton**，该网络在 2012 年的 ImageNet ILSVRC 图像识别竞赛中取得冠军，识别正确率领先第二名近 10%。\n",
    "\n",
    "AlexNet 模型共有八层结构，其中前五层为卷积层，其中前两个卷积层和第五个卷积层有池化层，最后三层为全连接层，整个网络模型共包含约六十五万个神经元，所需要训练的参数约 6000 万（60 M）个。\n",
    "\n",
    "![img](images/chapter14/AlexNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 神经网络的输入图像尺寸为 224×224×3\n",
    "\n",
    "\n",
    "- 第一层：卷积层1，卷积核尺寸为 11×11，填充 padding = 2，步长 stride = 4，卷积核的数量为 96（两块 GPU 分别计算 48 个核），输出图像分辨率为 55×55×96。然后接最大池化层，pool_size = (3, 3), stride = 2（带重叠的池化，Overlapping Pooling），输出图像分辨率为 27×27×96\n",
    "\n",
    "\n",
    "- 第二层：卷积层2，卷积核尺寸为 5×5，padding = 2, stride = 1，卷积核的数量为 256（两块 GPU 分别计算 128 个核），输出图像分辨率为 27×27×256。然后做 LRN（Local Response Normalization，局部响应归一化），再接最大池化层，pool_size = (3, 3), stride = 2，输出图像分辨率为 13×13×256\n",
    "\n",
    "\n",
    "- 第三层：卷积层3，卷积核尺寸为 3×3，padding = 1，卷积核的数量为 384（两块 GPU 分别计算 192 个核），输出图像分辨率为 13×13×384\n",
    "\n",
    "\n",
    "- 第四层：卷积层4，卷积核尺寸为 3×3，padding = 1，卷积核的数量为 384（两块 GPU 分别计算 192 个核），输出图像分辨率为 13×13×384\n",
    "\n",
    "\n",
    "- 第五层：卷积层5，卷积核尺寸为 3×3，padding = 1，卷积核的数量为 256（两块 GPU 分别计算 128 个核），输出图像分辨率为 13×13×256。然后接最大池化层，pool_size = (3, 3), stride = 2，输出图像分辨率为 6×6×256（共 9216 个神经元）\n",
    "\n",
    "\n",
    "- 第六七层是全连接层，每一层的神经元的个数为 4096\n",
    "\n",
    "\n",
    "- 最后一层输出层的神经元个数为 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|名称|类型|参数|输出图像尺寸|参数数量|\n",
    "|:--|:--|:--|:--|--:|\n",
    "|Input|输入层||224×224×3|0|\n",
    "|Conv1|**卷积层**|k=11,s=4,p=2|55×55×96|(11×11×3+1)×96=34,944|\n",
    "|Pool1|池化层|k=3,s=2|27×27×96|0|\n",
    "|Conv2|**卷积层**|k=5,s=1,p=2|27×27×256|(5×5×96+1)×256=614,656|\n",
    "|Pool2|池化层|k=3,s=2|13×13×256|0|\n",
    "|Conv3|**卷积层**|k=3,s=1,p=1|13×13×384|(3×3×256+1)×384=885,120|\n",
    "|Conv4|**卷积层**|k=3,s=1,p=1|13×13×384|(3×3×384+1)×384=1,327,488|\n",
    "|Conv5|**卷积层**|k=3,s=1,p=1|13×13×256|(3×3×384+1)×256=884,992|\n",
    "|Pool3|池化层|k=3,s=2|6×6×256|0|\n",
    "|FC6|**全连接层**|4096|4096|(6×6×256+1)×4096=37,752,832|\n",
    "|FC7|**全连接层**|4096|4096|(4096+1)×4096=16,781,312|\n",
    "|FC8|**全连接层**|1000|1000|(4096+1)×1000=4,097,000|\n",
    "|Softmax|**输出层**||1000|0|\n",
    "|||||总数：62,378,344|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将原文中基于两块 GPU 实现的网络结构变为单 GPU，如下图所示\n",
    "\n",
    "![img](images/chapter14/AlexNet_single_GPU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet 网络的特点：\n",
    "\n",
    "- 由 5 层卷积和 3 层全连接组成，输入图像尺寸为 3 通道224×224，网络规模远大于 LeNet\n",
    "\n",
    "\n",
    "- 成功使用 ReLU 激活函数，克服了 Sigmoid 在网络较深时的梯度消失现象\n",
    "\n",
    "\n",
    "- 使用了Dropout，p=0.5，可以防止过拟合\n",
    "\n",
    "\n",
    "- 大量使用数据增强技术（从256×256原始图像中裁剪出224×224图像，再配合上水平镜像，可以达到 **2048** 倍训练数据）\n",
    "\n",
    "\n",
    "- 使用 LRN 层（目前不再使用）\n",
    "\n",
    "\n",
    "- 使用两块GPU，分两组进行卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [VGGNet](https://arxiv.org/abs/1409.1556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet 网络模型中局部响应归一化层(LRN)浪费了计算资源，但是对性能却没有很大的提升。\n",
    "\n",
    "牛津大学的视觉几何组提出的 VGGNet 的实质是 AlexNet 结构的增强版，它侧重强调卷积神经网络设计中的深度，将卷积层的深度提升到了16/19层。在 ImageNet  数据集上将 Top-5 错误率减小到7.3%，在 2014 年的 ILSVRC 大赛中取得图像分类任务的第二名和定位任务的第一名。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG 网络专注于构建卷积层的简单网络，全部采用更小的 3×3 卷积核，而不是 AlexNet 中的 11×11 卷积核。两个连续的 3×3 的卷积核相当于 5×5 的感受野，三个 3×3 的连续的卷积核相当于 7×7 的感受野。\n",
    "\n",
    "![img](images/chapter14/VGG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，网络 D 也就是通常所说的 VGG 16 网络模型，也可以绘制成下图\n",
    "\n",
    "![img](images/chapter14/VGG16.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|名称|类型|参数|输出图像尺寸|参数数量|\n",
    "|:--|:--|:--|:--|--|\n",
    "|Input|输入层||224×224×3|0|\n",
    "|CONV 64|**卷积层**|k=3,s=1,p=1|224×224×64|(3×3×3+1)×64=1,792|\n",
    "|CONV 64|**卷积层**|k=3,s=1,p=1|224×224×64|(3×3×64+1)×64=36,928|\n",
    "|POOL 2|池化层|k=2,s=2|112×112×64|0|\n",
    "|CONV 128|**卷积层**|k=3,s=1,p=1|112×112×128|(3×3×64+1)×128=73,856|\n",
    "|CONV 128|**卷积层**|k=3,s=1,p=1|112×112×128|(3×3×128+1)×128=147,584|\n",
    "|POOL 2|池化层|k=2,s=2|56×56×128|0|\n",
    "|CONV 256|**卷积层**|k=3,s=1,p=1|56×56×256|(3×3×128+1)×256=295,168|\n",
    "|CONV 256|**卷积层**|k=3,s=1,p=1|56×56×256|(3×3×256+1)×256=590,080|\n",
    "|CONV 256|**卷积层**|k=3,s=1,p=1|56×56×256|(3×3×256+1)×256=590,080|\n",
    "|POOL 2|池化层|k=2,s=2|28×28×256|0|\n",
    "|CONV 512|**卷积层**|k=3,s=1,p=1|28×28×512|(3×3×256+1)×512=1,180,160|\n",
    "|CONV 512|**卷积层**|k=3,s=1,p=1|28×28×512|(3×3×512+1)×512=2,359,808|\n",
    "|CONV 512|**卷积层**|k=3,s=1,p=1|28×28×512|(3×3×512+1)×512=2,359,808|\n",
    "|POOL 2|池化层|k=2,s=2|14×14×512|0|\n",
    "|CONV 512|**卷积层**|k=3,s=1,p=1|14×14×512|(3×3×512+1)×512=2,359,808|\n",
    "|CONV 512|**卷积层**|k=3,s=1,p=1|14×14×512|(3×3×512+1)×512=2,359,808|\n",
    "|CONV 512|**卷积层**|k=3,s=1,p=1|14×14×512|(3×3×512+1)×512=2,359,808|\n",
    "|POOL 2|池化层|k=2,s=2|7×7×512|0|\n",
    "|FC|**全连接层**|4096|4096|**(7×7×512+1)×4096=102,764,544**|\n",
    "|FC|**全连接层**|4096|4096|(4096+1)×4096=16,781,312|\n",
    "|FC|**全连接层**|1000|1000|(4096+1)×1000=4,097,000|\n",
    "|Softmax|**输出层**||1000|0|\n",
    "|||||总数：138,357,544|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络参数数量（单位：百万）：\n",
    "\n",
    "|网络模型|A |A-LR|B|C|D|E|\n",
    "|--|--|--|--|--|--|--|\n",
    "|参数数量|133|133|133|134|138|144|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG 网络的特点：\n",
    "\n",
    "- 取消了局部响应归一化层\n",
    "\n",
    "\n",
    "- 网络结构规整\n",
    "\n",
    "\n",
    "- 使用 3×3 卷积核，多个小滤波器卷积层的叠加比一个大滤波器卷积层效果好\n",
    "\n",
    "\n",
    "- 每使用一次池化层，特征图通道数翻倍\n",
    "\n",
    "\n",
    "- 验证了通过不断加深网络结构可以提升性能\n",
    "\n",
    "\n",
    "- 迁移到其它数据集时泛化性能好\n",
    "\n",
    "\n",
    "- 模型参数数量非常庞大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [GoogLeNet](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "首先思考两个问题：\n",
    "\n",
    "- 神经网络除了纵向的扩展，是否能进行横向的拓展？\n",
    "\n",
    "\n",
    "- 在传统的网络中，每个层从前一层提取信息，以便从输入数据中提取更丰富的特征表示。然而，每个层类型提取不同种类的信息，例如 5×5 卷积核的输出与 3×3 卷积核的输出不同的特征图。在任何给定的层面，我们如何知道哪种转换提供了最有用的信息？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google 于 2014年 ILSVRC 大赛凭借 GoogLeNet 获得图像分类问题的冠军，这个网络模型通过同时增加卷积神经网络的深度和宽度获得了更好地效果。GoogLeNet 模型使用 **Inception** 模块来代替简单的卷积层。Inception 模块将 CNN 中常用的卷积运算（1×1，3×3，5×5）和池化操作拼接在一起（卷积、池化后的尺寸相同，将通道相加）。采用不同大小的卷积核意味着不同大小的感受野，拼接意味着不同尺度特征的融合。因此，Inception 模块一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。\n",
    "![img](images/chapter14/inception_naive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而原始版本 Inception 中所有的卷积核都对上一层的所有特征图来做，而 5×5 的卷积核所需的计算量太大。为了避免这种情况，在 3×3 卷积、5×5 卷积层之前和 max pooling 层之后分别加上了 1×1 的卷积层，以起到了降低特征图数量的作用。这也就形成了 Inception v1的网络结构，如下图所示：\n",
    "\n",
    "![img](images/chapter14/inception.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于 Inception 模块构建了 GoogLeNet 的网络结构共 22 层，如下所示\n",
    "\n",
    "![img](images/chapter14/GoogLeNet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet 网络模型的参数如下表所示，其中**“#3x3 reduce”**和**“#5x5 reduce”** 分别代表在 3x3 卷积核 5×5卷积之前的 **1x1 卷积核的个数**。整个网络参数个数约为 680 万（6.8 M）个。\n",
    "\n",
    "![img](images/chapter14/GoogLeNet_parameters.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet 网络的特点：\n",
    "\n",
    "- Inception 模块融合了不同尺度的特征图\n",
    "\n",
    "\n",
    "- 使用 1x1 卷积核降低维度，减少了参数数量\n",
    "\n",
    "\n",
    "- 网络采用了模块化的结构，方便增添和修改\n",
    "\n",
    "\n",
    "- 最后一个池化层使用平均池化（Average Pooling），将输出图像分辨率降为 1x1\n",
    "\n",
    "\n",
    "- 为了避免梯度消失，网络额外增加了 2 个辅助的 softmax 输出用于向前传导梯度\n",
    "\n",
    "\n",
    "- 网络后续进化出更加强大的 Inception [V2](https://arxiv.org/abs/1502.03167), [V3](https://arxiv.org/abs/1512.00567), [V4](https://arxiv.org/abs/1602.07261) 版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. [ResNet](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "2015年，微软亚洲研究院（MSRA）[何恺明（Kaiming He）](http://kaiminghe.com/)等人提出的 ResNet（Residual Network）夺得了 ILSVRC 大赛图像分类问题的冠军，将 Top-5 误差率降低至 3.5%。ResNet 的最重要特征在于具有比以前的网络更深的结构。\n",
    "\n",
    "加深层对于提升网络模型性能很重要。自 AlexNet 以来，最先进的 CNN 架构已经越来越深。AlexNet 只有 5 个卷积层，而之后的 VGG 网络和 GoogLeNet分别有 16/19 层和 22 层。但是在深度神经网络中，过度加深层的话容易带来梯度爆炸/消散的问题，很多情况下学习将不能顺利进行，导致最终性能不佳。\n",
    "\n",
    "![img](images/chapter14/plain_network_training_error.png)\n",
    "\n",
    "为了解决这类问题，ResNet 中导入了**“快捷路径”**（shortcut connection）。导入这个快捷路径后，就可以随着层的加深而不断提高性能了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图为 **残差模块**（Residual Block）示意图，其内部加了一条快捷路径，使得前面的输入可以直接连接到输出。原本网络模块要学习的是 $H(x)$，现在由于多加了输入 $x$ 的影响，网络层需要拟合的变成了 $F(x) = H(x) - x$。当浅层的 $x$ 代表的特征图已经足够成熟，如果任何对于特征 $x$ 的改变都会让 loss 增加的话，$F(x)$ 会自动趋向于学习成为 0。\n",
    "\n",
    "![img](images/chapter14/residual_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，向残差模块加入 1×1的卷积核用来增加非线性和减小输出的深度以减小计算成本，就得到了带有“瓶颈”（bottleneck）的残差模块。\n",
    "\n",
    "![img](images/chapter14/deeper_residual_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34 层的 ResNet 和 VGG19、Plain Network（不带快捷路径的网络） 模型进行对比。图中用不同颜色区分不同分辨率的特征图输出，虚线代表当残差网络模块的输入输出维度不一致时，需要使维度统一，因此要对维数较少的特征图进行増维（可以使用补零或 1×1 卷积实现）。\n",
    "\n",
    "![img](images/chapter14/ResNet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同深度的 ResNet 网络模型参数如下：\n",
    "\n",
    "![img](images/chapter14/ResNet_parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet 的特点：\n",
    "\n",
    "- 每个残差模块包含两个 3×3 卷积层或使用 1×1 卷积层构造瓶颈\n",
    "\n",
    "\n",
    "- 在每个卷积层后使用 BN（Batch Normalization）层\n",
    "\n",
    "\n",
    "- 周期性的使用步幅为 2 的卷积层降低特征图分辨率\n",
    "\n",
    "\n",
    "- 未使用 Dropout 层\n",
    "\n",
    "\n",
    "- 模型参数量少，用于 CIFAR-10 数据集分类的 110 层 ResNet 参数量仅有 170 万（1.7 M）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. [ResNeXt](https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "ResNeXt 网络模型同时采用 ResNet 的 shortcut connection 思想和 Inception 的 split-transform-merge 思想，在增加准确率的同时基本不改变或降低模型的复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/chapter14/ResNeXt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ResNet-50 和 ResNeXt-50 的内部结构对比：\n",
    "\n",
    "![img](images/chapter14/ResNeXt-50.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/chapter14/ResNeXt_result.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. [DenseNet](https://arxiv.org/abs/1608.06993)\n",
    "\n",
    "在 ResNet 模型中，通过增加恒等映射，提高了网络训练的效果与效率。令 $x_\\ell$ 是第 $\\ell$ 层的输出，则\n",
    "\n",
    "$$ x_\\ell = H_\\ell(x_{\\ell-1}) + x_{\\ell-1} $$\n",
    "\n",
    "Huang 等人于2016年在论文《Densely Connected Convolutional Networks》中提出一种新架构 DenseNet，进一步利用快捷路径，将所有层直接连接在一起。在这种新型架构中，每层的输入由所有之前层的特征图组成，其输出将传输给每个后续层。这些特征映射通过深度级联聚合。\n",
    "\n",
    "$$ x_\\ell = H_\\ell[x_0,x_1,...,x_{\\ell-1}] $$\n",
    "\n",
    "如果函数 $H_\\ell$ 输出 $k$ 个特征图，那么第 $\\ell$ 层的输入包含有 $k_0+k\\times(\\ell -1)$ 个特征图，其中 $k_0$ 是原始输入层的通道数。这里的 $k$ 被称为**增长率**（Growth rate）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/chapter14/DenseNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图为带有三个 Dense Block 的 DenseNet。相邻两个 Dense Block 中间为**过渡层**（Transition Layer），包含了一个 BN 层，一个 1×1 卷积层和 2×2 平均池化层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/chapter14/DenseNet_with_three_dense_blocks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下表列出了 DenseNet 用于 ImageNet 数据集的网络结构，其中增长率 $k=32$，每个“conv”层都由“BN-Relu-Conv”组合而成。\n",
    "\n",
    "![img](images/chapter14/DenseNet_architecture_for_ImageNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet 模型分类精度较 Resnet 稍好，而且参数量下降许多，但 DenseNet 在训练时会消耗更多的内存/显存，不过对于消耗内存较多的问题已经有一些[改进方法](https://arxiv.org/abs/1707.06990)（共享存储等方法）。\n",
    "\n",
    "![img](images/chapter14/DenseNet_vs_ResNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. [SENet](https://arxiv.org/abs/1709.01507)\n",
    "\n",
    "SENet（Squeeze-and-Excitation Networks）是由自动驾驶公司 Momenta 在2017年公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，将重要的特征进行强化来提升准确率。这个网络结构取得了2017 ILSVRC 竞赛的冠军，Top-5 错误率达到了2.251%。\n",
    "\n",
    "![img](images/chapter14/SENet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squeeze**：\n",
    "\n",
    "Squeeze 采用全局平均池化(Global Average Pooling)将逐个通道的特征图编码为一个全局特征，这个实数某种程度上具有全局的感受野。Squeeze 输出的维度和输入的特征通道数相等。\n",
    "\n",
    "$$z_c = {\\rm F}_{sq}(u_c) = \\frac{1}{H\\times W}\\sum_{i=1}^{H}\\sum_{j=1}^{W}u_c(i,j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excitation**：\n",
    "\n",
    "Excitation 目的是学习各个通道的权重系数，从而使得模型对各个通道的特征更有辨别能力。\n",
    "\n",
    "$${\\rm s} = {\\rm F}_{ex}(z, {\\rm W}) = \\sigma(g(z, {\\rm W})) = \\sigma({\\rm W}_2 \\delta({\\rm W}_1 z))$$\n",
    "\n",
    "其中 ${\\rm W}_1 \\in \\mathbb{R}^{\\frac{C}{r}\\times C}$， ${\\rm W}_2 \\in \\mathbb{R}^{C\\times \\frac{C}{r}}$，$\\delta$ 为 ReLU 激活函数，$\\sigma$ 为 Sigmoid 激活函数。为了降低模型复杂度以及提升泛化能力，网络采用包含两个全连接层的瓶颈结构，瓶颈的压缩比定义为 $r$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后 ${\\rm F}_{scale}$ 将学习到的各个通道的激活值（sigmoid激活）乘以 $U$ 上的原始特征\n",
    "\n",
    "$$ \\tilde{x}_c = {\\rm F}_{scale}(u_c, s_c)=s_c u_c $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE模块可以直接嵌入到现有的网络结构中，例如分别嵌入到 Inception 结构和 ResNet 结构中：\n",
    "\n",
    "![img](images/chapter14/SE-Inception_module.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/chapter14/SE-ResNet_module.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了验证SE模块的作用，论文中在常用网络结构引入SE模块，并测试其在 ImageNet 数据集上的分类准确度。对比 ResNet-50 和 SE-ResNet-50，以及 ResNeXt-50 和 SE-ResNeXt-50 的 Top-1 错误率如下图所示。\n",
    "\n",
    "![img](images/chapter14/SE-ResNet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结\n",
    "\n",
    "论文[《An Analysis of Deep Neural Network Models for Practical Applications》](https://arxiv.org/abs/1605.07678)对 2017 年以来的经典卷积神经网络进行了横向对比。\n",
    "\n",
    "图中纵坐标代表 ImageNet Top-1 分类精度。左图横坐标是不同网络模型，右图横坐标为模型的操作复杂度，圆形尺寸代表模型的参数规模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/chapter14/comparison.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
